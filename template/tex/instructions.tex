\maketitle % title page

% THANKS
We gratefully acknowledge the receipt of \numreceived{}
problems from the following individuals:
\begin{quote}
\large \thankedauthors
\end{quote}

\tableofcontents

\chapter*{Confidentiality Reminder}
The following is the list of reviewers.
These are the only people with whom you should discuss
the packet materials, unless otherwise authorized.
The shortlisted problems must remain confidential
even after the exam concludes because unused problems are returned
to the authors for possible submission to other contests.

\vspace{1em}

\lstinputlisting[numbers=none,basicstyle=\ttfamily\footnotesize]{names.txt}
\vfill
\begin{mdframed}
	\begin{center}
		\bfseries
		\Large
		{ \color{blue} ALL MATERIALS CONFIDENTIAL INDEFINITELY} \\[1em]
		Do not discuss problems with anyone not listed above
	\end{center}
\end{mdframed}
\vspace{1.5em}

\pagebreak

\chapter{Instructions for reviewers}
Thank you for agreeing to review this packet!
Submit your comments to
\begin{center}
	\submitURL
\end{center}
no later than \alert{\deadline}.
A few quick notes on the form:
\begin{itemize}
\ii This form will require a Google account to submit.

\ii If you sign in with the same account, even on multiple devices
or browsers, it will show all previous responses you submitted
and also let you edit the submission freely.
So feel free to submit your ratings liberally,
since you can go update them at any point.

\ii You should rate your own problems.
\end{itemize}

\section{Problem assignments}
For those reviewers that volunteered to do so,
I assigned a particular point person who is responsible
for trying each problem.
This is to ensure that every problem is
considered thoroughly by at least one person.

\lstinputlisting%
[basicstyle=\ttfamily,numbers=none]
{assign.txt}

%if this doesn't stay on page, your packet is too long

\newpage

\section{Notes on packet organization}
There are a couple things to note about the packet organization.
\begin{itemize}
	\ii Problems are sorted \textbf{randomly}
	within sections, not by difficulty.

	\ii The subject categories (A, C, G, N)
	are for organizational reasons only.
	They represent what the problems appear
	to be \emph{at first glance},
	not intended to say anything about the
	actual nature of the problems.
	This means for example a problem which looks
	like a number theory problem will be labeled by N,
	even if it is really combinatorics.
	In some cases when more than one label seems to apply,
	the choice was arbitrary.

	On the other hand, we still welcome feedback of the form
	``this problem looks like X but is actually more Y''.
\end{itemize}

\section{Quality ratings}
The quality rating shows how nice you think this is as a contest problem.
Here is a description of what each of the ratings means,
together with some examples of problems from recent exams
(based on the ratings given by reviewers).

\begin{description}
	\ii[Unsuitable] The problem can't be used;
	either there is a well-known reference,
	the problem is broken in some way,
	or it is way too inelegant.
	Think of this as a veto.

	\ii[Mediocre] This problem is usable,
	but it'd be better to select something else if possible
	(e.g., the problem may be somewhat boring or standard,
	or a similar but not identical problem is known).
	\begin{itemize}
		\ii TSTST 2017/4 on $2^a + 3^b + 5^c = n!$
	\end{itemize}

	\ii[Acceptable] This is a fine problem with no major defects.
	Perhaps not stellar, but decent and perfectly serviceable.
	\begin{itemize}
		\ii TSTST 2019/1 on $\diamondsuit$
		\ii USA TST 2020/1 on $b_n/n^2$
	\end{itemize}

	\ii[Nice] This is a genuinely good problem (could be easy)
	that you would root for.
	\begin{itemize}
		\ii USAMO 2019/3 on the digit $7$
		\ii USA TST 2019/2 on power sums
	\end{itemize}

	\ii[Excellent] This is an amazing problem that stands out;
	use for your ``favorite'' problems.
	\begin{itemize}
		\ii TSTST 2019/3 on cars
	\end{itemize}
\end{description}
The weights of these five ratings in aggregation
are $-0.75$, $-0.5$, $0$, $+1$, $+1.5$.
(These numbers are contrived, but they're not arbitrary;
I chose them after some simulations against past reviewer data.)

These ratings should be independent of difficulty.
So a problem can be rated ``excellent''
even if it is too easy for team selection
(but would be good for e.g.\ JMO).
On the other hand an edge case like this
should be noted in the general comments for that section.

\section{Difficulty ratings}
The checkboxes provided are IMO 1, IMO 2, IMO 3.
Checking a box indicates that the problem ``could be''
reasonably used in that slot for the IMO.
Because one can select two checkboxes for borderline problems,
this means that there are effectively \emph{five} difficulty ratings.

Here is a text description of each difficulty caliber,
as well as examples of recent IMO problems I think fit there
based both on my own prejudice and the actual statistics.
\begin{description}
	\ii[IMO1 only] 85\% or higher solve rate at MOP,
	USA IMO team should sweep.
	\begin{itemize}
		\ii IMO 2017/1 on $\sqrt{a_n}$ versus $a_n+3$.
		\ii IMO 2018/1 geometry with $\ol{DE} \parallel \ol{FG}$.
	\end{itemize}
	\ii[IMO1--IMO2] 70\% solve rate at MOP,
	USA IMO team should sweep.
	\begin{itemize}
		\ii IMO 2017/4 geometry on $KT$ tangent to $\Gamma$
		\ii IMO 2018/5 on $\frac{a_1}{a_2} + \dots + \frac{a_n}{a_1}$.
	\end{itemize}
	\ii[IMO2 only] 50\% solve rate at MOP,
	USA IMO team gets 4-6 solves.
	\begin{itemize}
		\ii IMO 2018/2 on $a_i a_{i+1} + 1 = a_{i+2}$.
	\end{itemize}
	\ii[IMO2--IMO3] 30\% solve rate at MOP,
	USA IMO team gets 2-4 solves.
	\begin{itemize}
		\ii IMO 2017/2 on $f(f(x)f(y))+f(x+y) = f(xy)$.
		\ii IMO 2017/5 on Sir Alex
	\end{itemize}
	\ii[IMO3 only] 15\% or less solve rate at MOP,
	USA IMO team gets 0-2 solves for non-geo.\footnote{In
		recent years, the USA IMO team has
		done very well on difficult geometry problems
		compared to the international norm.}
	\begin{itemize}
		% \ii IMO 2018/3 on anti-Pascal triangle
		\ii IMO 2018/6 on $\angle AXB + \angle CXD = 180\dg$.
		\ii IMO 2017/6 on homogeneous polynomial $f(x,y)$ through $S$
	\end{itemize}
	% not shown: hunter/rabbit, Horst/Queenie
\end{description}
These buckets were designed for problems
which fit in the typical IMO span.
If you think a problem is way too easy or way too hard
(so as to be unsuitable),
indicate that in the text comments for the problem.

\section{Comments}
Write anything you want! Common examples:
\begin{itemize}
	\ii Comparison to past problems
	(in particular, identifying known problems)
	\ii Elaborations on quality or difficulty ratings
	(e.g.\ ``took me over five hours to solve'')
	\ii Arguments for or against using a problem
	\ii Alternate solutions (or outlines)
	\ii Proposed alternate versions of problems
	\ii Suggestions to changes in wording for clarity
\end{itemize}


\section{Dream test}
If you can, I would value your \emph{dream test},
i.e.\ the choice of problems you would use
if you were making the final decision on the exams.

This is helpful context for seeing how people
feel that the various problems interact with each other,
and for seeing if any problems repeatedly show up.

\section{Email discussion}
There will be an email thread where reviewers
are invited to discuss the problems amongst each other.
When doing so, we've traditionally used \textbf{white text}
to hide spoilers.

The discussion is just for fun and not a required part
of the review process (i.e.\ you can ignore the discussion thread).
Most of the ``official'' input will be the data
taken from the review form.

\section{Final report later}
When the entire problem selection process is completed,
after the deadline, a report will be sent out to all reviewers.
It should contain the following:
\begin{itemize}
	\ii The aggregate ratings provided by all reviewers.
	\ii A short sentence or two from the hosts
	summarizing the overall feedback on that problem.
	\ii A long chapter detailing how the hosts ended up
	deciding on which problems to choose for the exam.
	\ii Edited and cleaned-up versions of the
	chosen problems and solutions.
\end{itemize}
