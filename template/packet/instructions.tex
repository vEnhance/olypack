\maketitle % title page

% THANKS
We gratefully acknowledge the receipt of \numreceived{}
problems from the following individuals:
\begin{quote}
\large \thankedauthors
\end{quote}

\tableofcontents

\chapter*{Confidentiality Reminder}
The following is the list of reviewers.
These are the only people with whom you should discuss
the packet materials, unless otherwise authorized.
The shortlisted problems must remain confidential
even after the exam concludes because unused problems are returned
to the authors for possible submission to other contests.

\vspace{1em}

\lstinputlisting[numbers=none,basicstyle=\ttfamily\footnotesize]{reviewers.txt}
\vfill
\begin{mdframed}
  \begin{center}
    \bfseries
    \Large
    { \color{blue} ALL MATERIALS CONFIDENTIAL INDEFINITELY} \\[1em]
    Do not discuss problems with anyone not listed above
  \end{center}
\end{mdframed}
\vspace{1.5em}

\pagebreak

\chapter{Instructions for reviewers}
Thank you for agreeing to review this packet!
Submit your comments to
\begin{center}
  \submitURL
\end{center}
no later than \alert{\deadline}.
A few quick notes on the form:
\begin{itemize}
\ii This form will require a Google account to submit.

\ii If you sign in with the same account, even on multiple devices
or browsers, it will show all previous responses you submitted
and also let you edit the submission freely.
So feel free to submit your ratings liberally,
since you can go update them at any point.

\ii You should rate your own problems.
\end{itemize}

\section{Notes on packet organization}
There are a couple things to note about the packet organization.
\begin{itemize}
  \ii Problems are sorted \textbf{randomly}
  within sections, not by difficulty.

  \ii The subject categories (A, C, G, N)
  are for organizational reasons only.
  They represent what the problems appear
  to be \emph{at first glance},
  not intended to say anything about the
  actual nature of the problems.
  This means for example a problem which looks
  like a number theory problem will be labeled by N,
  even if it is really combinatorics.
  In some cases when more than one label seems to apply,
  the choice was arbitrary.

  On the other hand, we still welcome feedback of the form
  ``this problem looks like X but is actually more Y''.
\end{itemize}

\section{Quality ratings}
The quality rating shows how nice you think this is as a contest problem.
Here is a description of what each of the ratings means,
together with some examples of problems from recent exams
(based on the ratings given by reviewers where applicable).

\begin{description}
  \ii[Unsuitable] The problem can't be used;
  either there is a well-known reference,
  the problem is broken in some way,
  or it is way too inelegant.
  Think of this as a veto.

  \ii[Mediocre] This problem is usable,
  but it'd be better to select something else if possible
  (e.g., the problem may be somewhat boring or standard,
  or a similar but not identical problem is known).
  \begin{itemize}
    \ii IMO 2020/2 on $(a+2b+3c+4d)a^ab^bc^cd^d$
  \end{itemize}

  \ii[Acceptable] This is a fine problem with no major defects.
  Perhaps not stellar, but decent and perfectly serviceable.
  \begin{itemize}
    \ii USA TST 2023/5 on zero-sum $m\times n$ rectangles
    \ii USA TST 2020/1 on $b_n/n^2$
  \end{itemize}

  \ii[Nice] This is a genuinely good problem (could be easy)
  that you would root for.
  \begin{itemize}
    \ii TSTST 2021/9 on primitive root sum
    \ii USA TST 2023/1 on longest cycle on a circle
  \end{itemize}

  \ii[Excellent] This is an amazing problem that stands out;
  use for your ``favorite'' problems.
  \begin{itemize}
    \ii TSTST 2019/3 on cars
    \ii TSTST 2021/6 on triangles sharing circumcircle and incircle
  \end{itemize}
\end{description}
The weights of these five ratings in aggregation
are $-0.75$, $-0.5$, $0$, $+1$, $+1.5$.
(These numbers are contrived, but they're not arbitrary;
I chose them after some simulations against past reviewer data.)

These ratings should be independent of difficulty.
So a problem can be rated ``excellent''
even if it is too easy for team selection
(but would be good for e.g.\ JMO).
On the other hand an edge case like this
should be noted in the general comments for that section.

\section{Difficulty ratings}
The checkboxes provided are IMO 1, IMO 2, IMO 3.
Checking a box indicates that the problem ``could be''
reasonably used in that slot for the IMO.
Because one can select two checkboxes for borderline problems,
this means that there are effectively \emph{five} difficulty ratings.

Here is a text description of each difficulty caliber,
as well as examples of recent problems we think fit there
based our own prejudices and the actual statistics.
\begin{description}
  \ii[IMO1 only] 80\% or higher solve rate at MOP,
  USA IMO team should sweep.
  \begin{itemize}
    \ii IMO 2022/1 on ordering coins
    \ii IMO 2022/4 on $REAS$ collinear
  \end{itemize}
  \ii[IMO1--IMO2] 60\% solve rate at MOP,
  USA IMO team should sweep.
  \begin{itemize}
    \ii IMO 2022/2 on $xf(y)+yf(x)\leq 2$
    \ii TSTST 2022/7 on parallelogram and $KE=KF$
  \end{itemize}
  \ii[IMO2 only] 40\% solve rate at MOP,
  USA IMO team gets 4-6 solves.
  \begin{itemize}
    \ii IMO 2021/5 on Bushy and Jumpy
    \ii TSTST 2021/2 on unique $a_i/i$
  \end{itemize}
  \ii[IMO2--IMO3] 15\% solve rate at MOP,
  USA IMO team gets 2-4 solves.
  \begin{itemize}
    \ii TSTST 2022/6 on $K_AK_BK_CO$ cyclic
    \ii TST 2023/5 on zero-sum $m\times n$ rectangles
  \end{itemize}
  \ii[IMO3 only] 5\% or less solve rate at MOP,
  USA IMO team gets 0-2 solves.
  \begin{itemize}
    \ii IMO 2022/3 on $x^2+x+k$
    \ii TST 2023/3 on $g(0)+\dots+g(6000)$
  \end{itemize}
\end{description}
These buckets were designed for problems
which fit in the typical IMO span.
If you think a problem is way too easy or way too hard
(so as to be unsuitable),
indicate that in the text comments for the problem.

\section{Comments}
Write anything you want! Common examples:
\begin{itemize}
  \ii Comparison to past problems
  (in particular, identifying known problems)
  \ii Elaborations on quality or difficulty ratings
  (e.g.\ ``took me over five hours to solve'')
  \ii Arguments for or against using a problem
  \ii Alternate solutions (or outlines)
  \ii Proposed alternate versions of problems
  \ii Suggestions to changes in wording for clarity
\end{itemize}


\section{Dream test}
If you can, we would value your \emph{dream test},
i.e.\ the choice of problems you would use
if you were making the final decision on the exams.

This is helpful context for seeing how people
feel that the various problems interact with each other,
and for seeing if any problems repeatedly show up.

\section{Email discussion}
There will be an email thread where reviewers
are invited to discuss the problems amongst each other.
When doing so, we've traditionally used \textbf{white text}
to hide spoilers.

The discussion is just for fun and not a required part
of the review process (i.e.\ you can ignore the discussion thread).
Most of the ``official'' input will be the data
taken from the review form.

\section{Final report later}
When the entire problem selection process is completed,
after the deadline, a report will be sent out to all reviewers.
It should contain the following:
\begin{itemize}
  \ii The aggregate ratings provided by all reviewers.
  \ii A short sentence or two from the hosts
  summarizing the overall feedback on that problem.
  \ii A long chapter detailing how the hosts ended up
  deciding on which problems to choose for the exam.
  \ii Edited and cleaned-up versions of the
  chosen problems and solutions.
\end{itemize}
